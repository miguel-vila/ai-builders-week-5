{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b671d-420e-4973-9d6c-ad0a8e3c8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73cdf3aa4ae46eda7f08bb24bd815d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450f55567f6d4fda9eaf4dde517a5f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c615f0faf746ecad1465ea619c8a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds True\n",
      "encoder.embeddings.word_embeddings.weight False\n",
      "encoder.embeddings.position_embeddings.weight False\n",
      "encoder.embeddings.token_type_embeddings.weight False\n",
      "encoder.embeddings.LayerNorm.weight False\n",
      "encoder.embeddings.LayerNorm.bias False\n",
      "encoder.encoder.layer.0.attention.self.query.weight False\n",
      "encoder.encoder.layer.0.attention.self.query.bias False\n",
      "encoder.encoder.layer.0.attention.self.key.weight False\n",
      "encoder.encoder.layer.0.attention.self.key.bias False\n",
      "encoder.encoder.layer.0.attention.self.value.weight False\n",
      "encoder.encoder.layer.0.attention.self.value.bias False\n",
      "encoder.encoder.layer.0.attention.output.dense.weight False\n",
      "encoder.encoder.layer.0.attention.output.dense.bias False\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.0.intermediate.dense.weight False\n",
      "encoder.encoder.layer.0.intermediate.dense.bias False\n",
      "encoder.encoder.layer.0.output.dense.weight False\n",
      "encoder.encoder.layer.0.output.dense.bias False\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.1.attention.self.query.weight False\n",
      "encoder.encoder.layer.1.attention.self.query.bias False\n",
      "encoder.encoder.layer.1.attention.self.key.weight False\n",
      "encoder.encoder.layer.1.attention.self.key.bias False\n",
      "encoder.encoder.layer.1.attention.self.value.weight False\n",
      "encoder.encoder.layer.1.attention.self.value.bias False\n",
      "encoder.encoder.layer.1.attention.output.dense.weight False\n",
      "encoder.encoder.layer.1.attention.output.dense.bias False\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.1.intermediate.dense.weight False\n",
      "encoder.encoder.layer.1.intermediate.dense.bias False\n",
      "encoder.encoder.layer.1.output.dense.weight False\n",
      "encoder.encoder.layer.1.output.dense.bias False\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.2.attention.self.query.weight False\n",
      "encoder.encoder.layer.2.attention.self.query.bias False\n",
      "encoder.encoder.layer.2.attention.self.key.weight False\n",
      "encoder.encoder.layer.2.attention.self.key.bias False\n",
      "encoder.encoder.layer.2.attention.self.value.weight False\n",
      "encoder.encoder.layer.2.attention.self.value.bias False\n",
      "encoder.encoder.layer.2.attention.output.dense.weight False\n",
      "encoder.encoder.layer.2.attention.output.dense.bias False\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.2.intermediate.dense.weight False\n",
      "encoder.encoder.layer.2.intermediate.dense.bias False\n",
      "encoder.encoder.layer.2.output.dense.weight False\n",
      "encoder.encoder.layer.2.output.dense.bias False\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.3.attention.self.query.weight False\n",
      "encoder.encoder.layer.3.attention.self.query.bias False\n",
      "encoder.encoder.layer.3.attention.self.key.weight False\n",
      "encoder.encoder.layer.3.attention.self.key.bias False\n",
      "encoder.encoder.layer.3.attention.self.value.weight False\n",
      "encoder.encoder.layer.3.attention.self.value.bias False\n",
      "encoder.encoder.layer.3.attention.output.dense.weight False\n",
      "encoder.encoder.layer.3.attention.output.dense.bias False\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.3.intermediate.dense.weight False\n",
      "encoder.encoder.layer.3.intermediate.dense.bias False\n",
      "encoder.encoder.layer.3.output.dense.weight False\n",
      "encoder.encoder.layer.3.output.dense.bias False\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.4.attention.self.query.weight False\n",
      "encoder.encoder.layer.4.attention.self.query.bias False\n",
      "encoder.encoder.layer.4.attention.self.key.weight False\n",
      "encoder.encoder.layer.4.attention.self.key.bias False\n",
      "encoder.encoder.layer.4.attention.self.value.weight False\n",
      "encoder.encoder.layer.4.attention.self.value.bias False\n",
      "encoder.encoder.layer.4.attention.output.dense.weight False\n",
      "encoder.encoder.layer.4.attention.output.dense.bias False\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.4.intermediate.dense.weight False\n",
      "encoder.encoder.layer.4.intermediate.dense.bias False\n",
      "encoder.encoder.layer.4.output.dense.weight False\n",
      "encoder.encoder.layer.4.output.dense.bias False\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.5.attention.self.query.weight False\n",
      "encoder.encoder.layer.5.attention.self.query.bias False\n",
      "encoder.encoder.layer.5.attention.self.key.weight False\n",
      "encoder.encoder.layer.5.attention.self.key.bias False\n",
      "encoder.encoder.layer.5.attention.self.value.weight False\n",
      "encoder.encoder.layer.5.attention.self.value.bias False\n",
      "encoder.encoder.layer.5.attention.output.dense.weight False\n",
      "encoder.encoder.layer.5.attention.output.dense.bias False\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.5.intermediate.dense.weight False\n",
      "encoder.encoder.layer.5.intermediate.dense.bias False\n",
      "encoder.encoder.layer.5.output.dense.weight False\n",
      "encoder.encoder.layer.5.output.dense.bias False\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.6.attention.self.query.weight False\n",
      "encoder.encoder.layer.6.attention.self.query.bias False\n",
      "encoder.encoder.layer.6.attention.self.key.weight False\n",
      "encoder.encoder.layer.6.attention.self.key.bias False\n",
      "encoder.encoder.layer.6.attention.self.value.weight False\n",
      "encoder.encoder.layer.6.attention.self.value.bias False\n",
      "encoder.encoder.layer.6.attention.output.dense.weight False\n",
      "encoder.encoder.layer.6.attention.output.dense.bias False\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.6.intermediate.dense.weight False\n",
      "encoder.encoder.layer.6.intermediate.dense.bias False\n",
      "encoder.encoder.layer.6.output.dense.weight False\n",
      "encoder.encoder.layer.6.output.dense.bias False\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.7.attention.self.query.weight False\n",
      "encoder.encoder.layer.7.attention.self.query.bias False\n",
      "encoder.encoder.layer.7.attention.self.key.weight False\n",
      "encoder.encoder.layer.7.attention.self.key.bias False\n",
      "encoder.encoder.layer.7.attention.self.value.weight False\n",
      "encoder.encoder.layer.7.attention.self.value.bias False\n",
      "encoder.encoder.layer.7.attention.output.dense.weight False\n",
      "encoder.encoder.layer.7.attention.output.dense.bias False\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.7.intermediate.dense.weight False\n",
      "encoder.encoder.layer.7.intermediate.dense.bias False\n",
      "encoder.encoder.layer.7.output.dense.weight False\n",
      "encoder.encoder.layer.7.output.dense.bias False\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.8.attention.self.query.weight False\n",
      "encoder.encoder.layer.8.attention.self.query.bias False\n",
      "encoder.encoder.layer.8.attention.self.key.weight False\n",
      "encoder.encoder.layer.8.attention.self.key.bias False\n",
      "encoder.encoder.layer.8.attention.self.value.weight False\n",
      "encoder.encoder.layer.8.attention.self.value.bias False\n",
      "encoder.encoder.layer.8.attention.output.dense.weight False\n",
      "encoder.encoder.layer.8.attention.output.dense.bias False\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.8.intermediate.dense.weight False\n",
      "encoder.encoder.layer.8.intermediate.dense.bias False\n",
      "encoder.encoder.layer.8.output.dense.weight False\n",
      "encoder.encoder.layer.8.output.dense.bias False\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.9.attention.self.query.weight False\n",
      "encoder.encoder.layer.9.attention.self.query.bias False\n",
      "encoder.encoder.layer.9.attention.self.key.weight False\n",
      "encoder.encoder.layer.9.attention.self.key.bias False\n",
      "encoder.encoder.layer.9.attention.self.value.weight False\n",
      "encoder.encoder.layer.9.attention.self.value.bias False\n",
      "encoder.encoder.layer.9.attention.output.dense.weight False\n",
      "encoder.encoder.layer.9.attention.output.dense.bias False\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.9.intermediate.dense.weight False\n",
      "encoder.encoder.layer.9.intermediate.dense.bias False\n",
      "encoder.encoder.layer.9.output.dense.weight False\n",
      "encoder.encoder.layer.9.output.dense.bias False\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.10.attention.self.query.weight False\n",
      "encoder.encoder.layer.10.attention.self.query.bias False\n",
      "encoder.encoder.layer.10.attention.self.key.weight False\n",
      "encoder.encoder.layer.10.attention.self.key.bias False\n",
      "encoder.encoder.layer.10.attention.self.value.weight False\n",
      "encoder.encoder.layer.10.attention.self.value.bias False\n",
      "encoder.encoder.layer.10.attention.output.dense.weight False\n",
      "encoder.encoder.layer.10.attention.output.dense.bias False\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.10.intermediate.dense.weight False\n",
      "encoder.encoder.layer.10.intermediate.dense.bias False\n",
      "encoder.encoder.layer.10.output.dense.weight False\n",
      "encoder.encoder.layer.10.output.dense.bias False\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.11.attention.self.query.weight False\n",
      "encoder.encoder.layer.11.attention.self.query.bias False\n",
      "encoder.encoder.layer.11.attention.self.key.weight False\n",
      "encoder.encoder.layer.11.attention.self.key.bias False\n",
      "encoder.encoder.layer.11.attention.self.value.weight False\n",
      "encoder.encoder.layer.11.attention.self.value.bias False\n",
      "encoder.encoder.layer.11.attention.output.dense.weight False\n",
      "encoder.encoder.layer.11.attention.output.dense.bias False\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.11.intermediate.dense.weight False\n",
      "encoder.encoder.layer.11.intermediate.dense.bias False\n",
      "encoder.encoder.layer.11.output.dense.weight False\n",
      "encoder.encoder.layer.11.output.dense.bias False\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias False\n",
      "encoder.pooler.dense.weight True\n",
      "encoder.pooler.dense.bias True\n",
      "shared_linear.weight True\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from dataclasses import dataclass\n",
    "import math, re, numpy as np\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    PreTrainedModel, PretrainedConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import evaluate\n",
    "\n",
    "# ---------- Config ----------\n",
    "MODEL_NAME = \"google-bert/bert-base-uncased\"\n",
    "MAX_LEN = 384\n",
    "NUM_BINS = 10                      # 0.5, 1.0, ..., 5.0  => 10 bins\n",
    "BIN_VALUES = np.arange(0.5, 5.0 + 0.5, 0.5)  # [0.5, 1.0, ..., 5.0]\n",
    "\n",
    "def rating_to_bin(r: float) -> int:\n",
    "    # map 0.5→0, 1.0→1, ..., 5.0→9\n",
    "    return int(round((r - 0.5) / 0.5))\n",
    "\n",
    "def bin_to_rating(b: int) -> float:\n",
    "    return 0.5 + 0.5 * b\n",
    "\n",
    "def class_to_cumulative_targets(y: torch.Tensor, num_bins: int) -> torch.Tensor:\n",
    "    # For class c, targets for thresholds k=0..K-2 are 1 if c > k else 0\n",
    "    # y: (B,) long\n",
    "    B = y.size(0)\n",
    "    k = torch.arange(num_bins - 1, device=y.device).unsqueeze(0).expand(B, -1)\n",
    "    return (y.unsqueeze(1) > k).float()  # (B, K-1)\n",
    "\n",
    "def preprocess(ex):\n",
    "    enc = tokenizer(ex[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    # map rating -> class 0..9\n",
    "    label = rating_to_bin(float(ex[\"rating\"]))\n",
    "    enc[\"labels\"] = label\n",
    "    return enc\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_set = pd.read_csv('./data/letterboxd_250movie_reviews_train.csv')\n",
    "val_set  = pd.read_csv('./data/letterboxd_250movie_reviews_val.csv')\n",
    "test_set  = pd.read_csv('./data/letterboxd_250movie_reviews_test.csv')\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_set),\n",
    "    'validation': Dataset.from_pandas(val_set),\n",
    "    'test': Dataset.from_pandas(test_set)\n",
    "})\n",
    "\n",
    "dataset = {k: v.map(preprocess, remove_columns=v.column_names) for k,v in dataset.items()}\n",
    "# dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# ---------- CORAL Model ----------\n",
    "class CoralConfig(PretrainedConfig):\n",
    "    model_type = \"coral\"\n",
    "    def __init__(self, base_model_name=MODEL_NAME, num_bins=NUM_BINS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name = base_model_name\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "class CoralForOrdinalRegression(PreTrainedModel):\n",
    "    config_class = CoralConfig\n",
    "\n",
    "    def __init__(self, config: CoralConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AutoModel.from_pretrained(config.base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Shared weight vector w (d->1), CORAL: logit_k = w^T h + b_k\n",
    "        self.shared_linear = nn.Linear(hidden, 1, bias=False)\n",
    "        self.thresholds = nn.Parameter(torch.zeros(config.num_bins - 1))\n",
    "        self.dropout = nn.Dropout(getattr(self.encoder.config, \"hidden_dropout_prob\", 0.1))\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "        # Get [CLS]-like pooled representation (use first token)\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # Use mean pooling of last hidden state → often more stable than CLS for some models\n",
    "        last = out.last_hidden_state  # (B, T, H)\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B, T, 1)\n",
    "        pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        s = self.shared_linear(pooled).squeeze(-1)                    # (B,)\n",
    "        logits = s.unsqueeze(1) + self.thresholds.unsqueeze(0)        # (B, K-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            targets = class_to_cumulative_targets(labels, self.config.num_bins)  # (B, K-1)\n",
    "            # BCEWithLogits over all thresholds\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"mean\")\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_classes(self, input_ids, attention_mask, token_type_ids=None, threshold: float = 0.5):\n",
    "        out = self.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        probs = torch.sigmoid(out[\"logits\"])           # (B, K-1)\n",
    "        # predicted class = count of thresholds passed (p_k > 0.5)\n",
    "        return (probs > threshold).sum(dim=1)          # (B,)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred.predictions is (B, K-1) logits\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds_class = (probs > 0.5).sum(axis=1)  # 0..9\n",
    "    true_class = labels\n",
    "\n",
    "    # Map to half-star ratings for MAE\n",
    "    preds_rating = np.array([bin_to_rating(int(c)) for c in preds_class])\n",
    "    true_rating  = np.array([bin_to_rating(int(c)) for c in true_class])\n",
    "\n",
    "    qwk = cohen_kappa_score(true_class, preds_class, weights=\"quadratic\")\n",
    "    mae = mean_absolute_error(true_rating, preds_rating)\n",
    "    acc = np.round(accuracy.compute(predictions=preds_rating, references=true_rating)['accuracy'],3)\n",
    "    return {\"qwk\": qwk, \"mae\": mae, \"acc\": acc}\n",
    "\n",
    "# ---------- Train ----------\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = CoralForOrdinalRegression(CoralConfig())\n",
    "\n",
    "# Freeze all base model params\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last encoder layer (optional, for better adaptation)\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if \"encoder.layer.11\" in name:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if \"pooler\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze CORAL head and thresholds\n",
    "model.shared_linear.weight.requires_grad = True\n",
    "model.thresholds.requires_grad = True\n",
    "\n",
    "# Print trainable status for all parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d4e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 09:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qwk</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680700</td>\n",
       "      <td>0.679369</td>\n",
       "      <td>0.247753</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.667801</td>\n",
       "      <td>0.359970</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.656538</td>\n",
       "      <td>0.411651</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.651159</td>\n",
       "      <td>0.416425</td>\n",
       "      <td>1.422500</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.646139</td>\n",
       "      <td>0.426262</td>\n",
       "      <td>1.395000</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.438493</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>0.639162</td>\n",
       "      <td>0.447676</td>\n",
       "      <td>1.357500</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.631500</td>\n",
       "      <td>0.636230</td>\n",
       "      <td>0.450721</td>\n",
       "      <td>1.367500</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>0.635494</td>\n",
       "      <td>0.453910</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.629600</td>\n",
       "      <td>0.635195</td>\n",
       "      <td>0.449754</td>\n",
       "      <td>1.362500</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.6453559551239013, metrics={'train_runtime': 564.8947, 'train_samples_per_second': 28.324, 'train_steps_per_second': 1.77, 'total_flos': 3136198358378592.0, 'train_loss': 0.6453559551239013, 'epoch': 10.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert-letterbox-reviews-classifier_teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d61b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qwk': 0.5639322133133833, 'mae': 1.2325, 'acc': np.float64(0.235)}\n"
     ]
    }
   ],
   "source": [
    "# apply model to validation dataset\n",
    "predictions = trainer.predict(dataset[\"test\"])\n",
    "\n",
    "# Extract the logits and labels from the predictions object\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Use your compute_metrics function\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
