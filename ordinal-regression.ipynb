{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334b671d-420e-4973-9d6c-ad0a8e3c8bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e427bf8bc51a40bc8e48894340a9490c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590371100e4b4ed5bddd04018fd39075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e98bd7ba594a82a90ab909e1d355e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds True\n",
      "encoder.embeddings.word_embeddings.weight False\n",
      "encoder.embeddings.LayerNorm.weight False\n",
      "encoder.embeddings.LayerNorm.bias False\n",
      "encoder.encoder.layer.0.attention.self.q_bias False\n",
      "encoder.encoder.layer.0.attention.self.v_bias False\n",
      "encoder.encoder.layer.0.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.0.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.0.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.0.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.0.attention.output.dense.weight False\n",
      "encoder.encoder.layer.0.attention.output.dense.bias False\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.0.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.0.intermediate.dense.weight False\n",
      "encoder.encoder.layer.0.intermediate.dense.bias False\n",
      "encoder.encoder.layer.0.output.dense.weight False\n",
      "encoder.encoder.layer.0.output.dense.bias False\n",
      "encoder.encoder.layer.0.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.0.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.1.attention.self.q_bias False\n",
      "encoder.encoder.layer.1.attention.self.v_bias False\n",
      "encoder.encoder.layer.1.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.1.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.1.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.1.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.1.attention.output.dense.weight False\n",
      "encoder.encoder.layer.1.attention.output.dense.bias False\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.1.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.1.intermediate.dense.weight False\n",
      "encoder.encoder.layer.1.intermediate.dense.bias False\n",
      "encoder.encoder.layer.1.output.dense.weight False\n",
      "encoder.encoder.layer.1.output.dense.bias False\n",
      "encoder.encoder.layer.1.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.1.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.2.attention.self.q_bias False\n",
      "encoder.encoder.layer.2.attention.self.v_bias False\n",
      "encoder.encoder.layer.2.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.2.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.2.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.2.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.2.attention.output.dense.weight False\n",
      "encoder.encoder.layer.2.attention.output.dense.bias False\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.2.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.2.intermediate.dense.weight False\n",
      "encoder.encoder.layer.2.intermediate.dense.bias False\n",
      "encoder.encoder.layer.2.output.dense.weight False\n",
      "encoder.encoder.layer.2.output.dense.bias False\n",
      "encoder.encoder.layer.2.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.2.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.3.attention.self.q_bias False\n",
      "encoder.encoder.layer.3.attention.self.v_bias False\n",
      "encoder.encoder.layer.3.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.3.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.3.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.3.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.3.attention.output.dense.weight False\n",
      "encoder.encoder.layer.3.attention.output.dense.bias False\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.3.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.3.intermediate.dense.weight False\n",
      "encoder.encoder.layer.3.intermediate.dense.bias False\n",
      "encoder.encoder.layer.3.output.dense.weight False\n",
      "encoder.encoder.layer.3.output.dense.bias False\n",
      "encoder.encoder.layer.3.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.3.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.4.attention.self.q_bias False\n",
      "encoder.encoder.layer.4.attention.self.v_bias False\n",
      "encoder.encoder.layer.4.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.4.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.4.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.4.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.4.attention.output.dense.weight False\n",
      "encoder.encoder.layer.4.attention.output.dense.bias False\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.4.intermediate.dense.weight False\n",
      "encoder.encoder.layer.4.intermediate.dense.bias False\n",
      "encoder.encoder.layer.4.output.dense.weight False\n",
      "encoder.encoder.layer.4.output.dense.bias False\n",
      "encoder.encoder.layer.4.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.4.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.5.attention.self.q_bias False\n",
      "encoder.encoder.layer.5.attention.self.v_bias False\n",
      "encoder.encoder.layer.5.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.5.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.5.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.5.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.5.attention.output.dense.weight False\n",
      "encoder.encoder.layer.5.attention.output.dense.bias False\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.5.intermediate.dense.weight False\n",
      "encoder.encoder.layer.5.intermediate.dense.bias False\n",
      "encoder.encoder.layer.5.output.dense.weight False\n",
      "encoder.encoder.layer.5.output.dense.bias False\n",
      "encoder.encoder.layer.5.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.5.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.6.attention.self.q_bias False\n",
      "encoder.encoder.layer.6.attention.self.v_bias False\n",
      "encoder.encoder.layer.6.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.6.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.6.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.6.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.6.attention.output.dense.weight False\n",
      "encoder.encoder.layer.6.attention.output.dense.bias False\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.6.intermediate.dense.weight False\n",
      "encoder.encoder.layer.6.intermediate.dense.bias False\n",
      "encoder.encoder.layer.6.output.dense.weight False\n",
      "encoder.encoder.layer.6.output.dense.bias False\n",
      "encoder.encoder.layer.6.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.6.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.7.attention.self.q_bias False\n",
      "encoder.encoder.layer.7.attention.self.v_bias False\n",
      "encoder.encoder.layer.7.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.7.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.7.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.7.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.7.attention.output.dense.weight False\n",
      "encoder.encoder.layer.7.attention.output.dense.bias False\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.7.intermediate.dense.weight False\n",
      "encoder.encoder.layer.7.intermediate.dense.bias False\n",
      "encoder.encoder.layer.7.output.dense.weight False\n",
      "encoder.encoder.layer.7.output.dense.bias False\n",
      "encoder.encoder.layer.7.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.7.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.8.attention.self.q_bias False\n",
      "encoder.encoder.layer.8.attention.self.v_bias False\n",
      "encoder.encoder.layer.8.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.8.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.8.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.8.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.8.attention.output.dense.weight False\n",
      "encoder.encoder.layer.8.attention.output.dense.bias False\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.8.intermediate.dense.weight False\n",
      "encoder.encoder.layer.8.intermediate.dense.bias False\n",
      "encoder.encoder.layer.8.output.dense.weight False\n",
      "encoder.encoder.layer.8.output.dense.bias False\n",
      "encoder.encoder.layer.8.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.8.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.9.attention.self.q_bias False\n",
      "encoder.encoder.layer.9.attention.self.v_bias False\n",
      "encoder.encoder.layer.9.attention.self.in_proj.weight False\n",
      "encoder.encoder.layer.9.attention.self.pos_proj.weight False\n",
      "encoder.encoder.layer.9.attention.self.pos_q_proj.weight False\n",
      "encoder.encoder.layer.9.attention.self.pos_q_proj.bias False\n",
      "encoder.encoder.layer.9.attention.output.dense.weight False\n",
      "encoder.encoder.layer.9.attention.output.dense.bias False\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.9.intermediate.dense.weight False\n",
      "encoder.encoder.layer.9.intermediate.dense.bias False\n",
      "encoder.encoder.layer.9.output.dense.weight False\n",
      "encoder.encoder.layer.9.output.dense.bias False\n",
      "encoder.encoder.layer.9.output.LayerNorm.weight False\n",
      "encoder.encoder.layer.9.output.LayerNorm.bias False\n",
      "encoder.encoder.layer.10.attention.self.q_bias True\n",
      "encoder.encoder.layer.10.attention.self.v_bias True\n",
      "encoder.encoder.layer.10.attention.self.in_proj.weight True\n",
      "encoder.encoder.layer.10.attention.self.pos_proj.weight True\n",
      "encoder.encoder.layer.10.attention.self.pos_q_proj.weight True\n",
      "encoder.encoder.layer.10.attention.self.pos_q_proj.bias True\n",
      "encoder.encoder.layer.10.attention.output.dense.weight True\n",
      "encoder.encoder.layer.10.attention.output.dense.bias True\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.10.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.10.intermediate.dense.weight True\n",
      "encoder.encoder.layer.10.intermediate.dense.bias True\n",
      "encoder.encoder.layer.10.output.dense.weight True\n",
      "encoder.encoder.layer.10.output.dense.bias True\n",
      "encoder.encoder.layer.10.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.10.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.11.attention.self.q_bias True\n",
      "encoder.encoder.layer.11.attention.self.v_bias True\n",
      "encoder.encoder.layer.11.attention.self.in_proj.weight True\n",
      "encoder.encoder.layer.11.attention.self.pos_proj.weight True\n",
      "encoder.encoder.layer.11.attention.self.pos_q_proj.weight True\n",
      "encoder.encoder.layer.11.attention.self.pos_q_proj.bias True\n",
      "encoder.encoder.layer.11.attention.output.dense.weight True\n",
      "encoder.encoder.layer.11.attention.output.dense.bias True\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.11.attention.output.LayerNorm.bias True\n",
      "encoder.encoder.layer.11.intermediate.dense.weight True\n",
      "encoder.encoder.layer.11.intermediate.dense.bias True\n",
      "encoder.encoder.layer.11.output.dense.weight True\n",
      "encoder.encoder.layer.11.output.dense.bias True\n",
      "encoder.encoder.layer.11.output.LayerNorm.weight True\n",
      "encoder.encoder.layer.11.output.LayerNorm.bias True\n",
      "encoder.encoder.rel_embeddings.weight False\n",
      "shared_linear.weight True\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from dataclasses import dataclass\n",
    "import math, re, numpy as np\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    PreTrainedModel, PretrainedConfig,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import evaluate\n",
    "\n",
    "# ---------- Config ----------\n",
    "MODEL_NAME = \"microsoft/deberta-base-mnli\"\n",
    "MAX_LEN = 384\n",
    "NUM_BINS = 10                      # 0.5, 1.0, ..., 5.0  => 10 bins\n",
    "BIN_VALUES = np.arange(0.5, 5.0 + 0.5, 0.5)  # [0.5, 1.0, ..., 5.0]\n",
    "\n",
    "def rating_to_bin(r: float) -> int:\n",
    "    # map 0.5→0, 1.0→1, ..., 5.0→9\n",
    "    return int(round((r - 0.5) / 0.5))\n",
    "\n",
    "def bin_to_rating(b: int) -> float:\n",
    "    return 0.5 + 0.5 * b\n",
    "\n",
    "def class_to_cumulative_targets(y: torch.Tensor, num_bins: int) -> torch.Tensor:\n",
    "    # For class c, targets for thresholds k=0..K-2 are 1 if c > k else 0\n",
    "    # y: (B,) long\n",
    "    B = y.size(0)\n",
    "    k = torch.arange(num_bins - 1, device=y.device).unsqueeze(0).expand(B, -1)\n",
    "    return (y.unsqueeze(1) > k).float()  # (B, K-1)\n",
    "\n",
    "def preprocess(ex):\n",
    "    enc = tokenizer(ex[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    # map rating -> class 0..9\n",
    "    label = rating_to_bin(float(ex[\"rating\"]))\n",
    "    enc[\"labels\"] = label\n",
    "    return enc\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_set = pd.read_csv('./data/letterboxd_250movie_reviews_train.csv')\n",
    "val_set  = pd.read_csv('./data/letterboxd_250movie_reviews_val.csv')\n",
    "test_set  = pd.read_csv('./data/letterboxd_250movie_reviews_test.csv')\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_set),\n",
    "    'validation': Dataset.from_pandas(val_set),\n",
    "    'test': Dataset.from_pandas(test_set)\n",
    "})\n",
    "\n",
    "dataset = {k: v.map(preprocess) for k,v in dataset.items()}\n",
    "# dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# ---------- CORAL Model ----------\n",
    "class CoralConfig(PretrainedConfig):\n",
    "    model_type = \"coral\"\n",
    "    def __init__(self, base_model_name=MODEL_NAME, num_bins=NUM_BINS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.base_model_name = base_model_name\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "class CoralForOrdinalRegression(PreTrainedModel):\n",
    "    config_class = CoralConfig\n",
    "\n",
    "    def __init__(self, config: CoralConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = AutoModel.from_pretrained(config.base_model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        # Shared weight vector w (d->1), CORAL: logit_k = w^T h + b_k\n",
    "        self.shared_linear = nn.Linear(hidden, 1, bias=False)\n",
    "        self.thresholds = nn.Parameter(torch.zeros(config.num_bins - 1))\n",
    "        self.dropout = nn.Dropout(getattr(self.encoder.config, \"hidden_dropout_prob\", 0.1))\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "        # Get [CLS]-like pooled representation (use first token)\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # Use mean pooling of last hidden state → often more stable than CLS for some models\n",
    "        last = out.last_hidden_state  # (B, T, H)\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B, T, 1)\n",
    "        pooled = (last * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        s = self.shared_linear(pooled).squeeze(-1)                    # (B,)\n",
    "        logits = s.unsqueeze(1) + self.thresholds.unsqueeze(0)        # (B, K-1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            targets = class_to_cumulative_targets(labels, self.config.num_bins)  # (B, K-1)\n",
    "            # BCEWithLogits over all thresholds\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"mean\")\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_classes(self, input_ids, attention_mask, token_type_ids=None, threshold: float = 0.5):\n",
    "        out = self.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        probs = torch.sigmoid(out[\"logits\"])           # (B, K-1)\n",
    "        # predicted class = count of thresholds passed (p_k > 0.5)\n",
    "        return (probs > threshold).sum(dim=1)          # (B,)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred.predictions is (B, K-1) logits\n",
    "    logits, labels = eval_pred\n",
    "    # make sure labels all fall in 0..9\n",
    "    if labels.min() < 0 or labels.max() >= NUM_BINS:\n",
    "        raise ValueError(f\"Labels should be in the range [0, {NUM_BINS-1}]\")\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds_class = (probs > 0.5).sum(axis=1)  # 0..9\n",
    "    true_class = labels\n",
    "\n",
    "    # Map to half-star ratings for MAE\n",
    "    preds_rating = np.array([bin_to_rating(int(c)) for c in preds_class])\n",
    "    true_rating  = np.array([bin_to_rating(int(c)) for c in true_class])\n",
    "\n",
    "    qwk = cohen_kappa_score(true_class, preds_class, weights=\"quadratic\")\n",
    "    mae = mean_absolute_error(true_rating, preds_rating)\n",
    "    acc = np.round(accuracy.compute(predictions=preds_rating, references=true_rating)['accuracy'],3)\n",
    "    return {\"qwk\": qwk, \"mae\": mae, \"acc\": acc}\n",
    "\n",
    "# ---------- Train ----------\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "model = CoralForOrdinalRegression(CoralConfig())\n",
    "\n",
    "# Freeze all base model params\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "encode_layers_to_unfreeze = [10, 11]\n",
    "\n",
    "# Unfreeze last encoder layer (optional, for better adaptation)\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if any([f\"encoder.layer.{l}\" in name for l in encode_layers_to_unfreeze]):\n",
    "        param.requires_grad = True\n",
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    if \"pooler\" in name: # for bert models\n",
    "        param.requires_grad = True\n",
    "\n",
    "# for name, param in model.encoder.named_parameters():\n",
    "#     if \"encoder.rel_embeddings.weight\" in name: #for DeBERTa models\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# Unfreeze CORAL head and thresholds, for bert models\n",
    "model.shared_linear.weight.requires_grad = True\n",
    "model.thresholds.requires_grad = True\n",
    "\n",
    "# Print trainable status for all parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d4e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 30:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Qwk</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638000</td>\n",
       "      <td>0.649151</td>\n",
       "      <td>0.451506</td>\n",
       "      <td>1.575000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.602843</td>\n",
       "      <td>0.522322</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535400</td>\n",
       "      <td>0.598580</td>\n",
       "      <td>0.494781</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.607922</td>\n",
       "      <td>0.495400</td>\n",
       "      <td>1.442500</td>\n",
       "      <td>0.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.623574</td>\n",
       "      <td>0.517182</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.619134</td>\n",
       "      <td>0.463664</td>\n",
       "      <td>1.472500</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.622693</td>\n",
       "      <td>0.473825</td>\n",
       "      <td>1.485000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.490067</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.618725</td>\n",
       "      <td>0.496292</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.453100</td>\n",
       "      <td>0.614531</td>\n",
       "      <td>0.478206</td>\n",
       "      <td>1.457500</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5066825103759766, metrics={'train_runtime': 1851.0898, 'train_samples_per_second': 8.644, 'train_steps_per_second': 0.27, 'total_flos': 3657219163310592.0, 'train_loss': 0.5066825103759766, 'epoch': 10.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert-letterbox-reviews-ordinal-regression_teacher\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"qwk\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d61b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qwk': 0.5589545796318471, 'mae': 1.355, 'acc': np.float64(0.2)}\n"
     ]
    }
   ],
   "source": [
    "# apply model to validation dataset\n",
    "predictions = trainer.predict(dataset[\"test\"])\n",
    "\n",
    "# Extract the logits and labels from the predictions object\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Use your compute_metrics function\n",
    "metrics = compute_metrics((logits, labels))\n",
    "print(metrics)\n",
    "\n",
    "# google-bert/bert-base-uncased\n",
    "#  200 samples per rating + only pooler unfrozen\n",
    "#  {'qwk': 0.5639322133133833, 'mae': 1.2325, 'acc': np.float64(0.235)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "#  200 samples per rating + pooler and 11th layers unfrozen\n",
    "#  {'qwk': 0.5772998908234213, 'mae': 1.305, 'acc': np.float64(0.205)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "#  200 samples per rating + pooler, 10 and 11th layers unfrozen\n",
    "#  {'qwk': 0.5454352351416798, 'mae': 1.35, 'acc': np.float64(0.205)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "# microsoft/deberta-base-mnli\n",
    "#  200 samples per rating + 11th layer and unfrozen\n",
    "#  {'qwk': 0.6079358581652387, 'mae': 1.27, 'acc': np.float64(0.205)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "#  200 samples per rating + 10th and 11th layer and unfrozen\n",
    "#  {'qwk': 0.6149355764361673, 'mae': 1.255, 'acc': np.float64(0.21)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "#  200 samples per rating + 9th, 10th and 11th layer and pooler\n",
    "#  {'qwk': 0.554329071675058, 'mae': 1.3425, 'acc': np.float64(0.21)} lr = 2e-4 batch_size = 16 num_epochs = 10\n",
    "#  200 samples per rating + 9th, 10th and 11th layer and pooler\n",
    "#  {'qwk': 0.605914123300682, 'mae': 1.26, 'acc': np.float64(0.2)} lr = 2e-4 batch_size = 24 num_epochs = 10\n",
    "#  {'qwk': 0.5589545796318471, 'mae': 1.355, 'acc': np.float64(0.2)} lr = 2e-4 batch_size = 32 num_epochs = 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a02a9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelvilagonzalez/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: oppenheimer-2023\n",
      "Text: idk if i have any large overarching thoughts yet so once again it is time for a scattered list!!!!!!!!!!!!!- truly goofy sex scenes. why is florence pugh taking a break from riding cillian murphy to pull a sanskrit book off his shelf and making him read it to her? AND absolutely smoothbrained to have her point at a random line, say, \"read it\" and it's literally \"i am become death, destroyer of worlds\" like fuck offffff with your obvious shit lol- no character in this movie is developed very deeply beyond the guy himself but florence pugh being here just to show tits and be a communist then die because apparently she's also unspecifically mentally ill is like....how are we still this bad at character christopher!!! when will you learn that a girlie hating flowers does not count as depth!!!! - last flo-related point but whoever decided cillian murphy should have his legs crossed in that scene is a coward- many weird and funny cameos but i think my favorite is rodrick rules himself bent over a bike vomiting with snot pouring out of his face. - bitch i love to see david krumholtz in a movie!!! my king!! you were so good in LEOPOLDSTADT!!!- sound mix is actually dreadful. too bass-heavy and muddled and often dominated by music to an extent that i kept missing what people were saying—and besties this movie is talky as fuck and full of quantum physics jargon, you gotta help me out a little bit here by making sure i can at least decipher everything that's being said. idk what happened here!- something very funny to me about my screening is that i was sitting second row in an imax for this movie and one of the trailers that played beforehand was for this paul giamatti alexander payne movie that ends with a freeze frame of paul giamatti, and i think because the movie is not supposed to be in imax, it cut off giamatti's head at the top and bottom so it showed him in close-up from eyebrows to chin just frozen mid-scream, and it held on his face for like 10 seconds and it was so scary to me that i never reached that level of adrenaline during any other point of OPPENHEIMER. - honestly? i was skeptical about RDJ even being in this movie but i thought he was excellent! his persona simply melted! benny safdie? doing too much i'm afraid! matt damon? maybe not doing enough? love to see dane dehaan out here wearing some little glasses.idk the sound mix was so annoying that after a while i started treating it like a music video that was occasionally interrupted by hearings and testimony and people shouting about atoms. the way this is structured and edited seems like nolan is so afraid of people being bored by it, it's absolutely frantic and not in a way that i find compelling (that way being like how orson welles cut THE TRIAL or F FOR FAKE or even THE OTHER SIDE OF THE WIND). the editing overall is totally overwrought, desperate to cover all things said and all people saying them, while also, like, having whole exchanges that are cut shot-reverse with totally disorienting 180-jumps that don't feel intentional even a little bit?? which is not me saying \"i know better than to do this\" so much as it feels some scenes were very intentionally shot and edited and other parts of this feel like it was trimmed within an inch of its life to get to that 180-minute runtime. so i guess i'm arriving at some kind of overarching thought finally which is that i didn't feel much of anything about this but i certainly wasn't bored. movie interesting! landing on \"you have started a chain reaction that will destroy the world\" is uhh a cool place to leave my brain considering the world is on fire constantly now and i'm always thinking about how we're all going to die from climate change eventually but yeah idk. i enjoyed the AC this afternoon.\n",
      "True rating: 2.5, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: army-of-shadows\n",
      "Text: I'm sorry, did I watch a different film than everyone else? This was boring as hell, I can't count how many times I fell asleep and how many times I looked away while it was on. I didn't even finish it so I shouldn't even be writing this, but there's no way I can watch the rest. I'll probably give it a chance in the future, but for now I would rather forget about it.\n",
      "True rating: 1.5, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: mary-and-max\n",
      "Text: A niche stop motion animation that takes a really personal subject for a lot of people sounds as a promising project; yet I found this film insoportable from start to finish. This film tries too hard to be funny but is just way too quirky and in several stances asinine that its intentions of \"comedy\" are a pain in the ass; maybe this could be considered a subjective aspect, I mean while I believe that comedy has a lineament to determine whether it is acceptable or it isn't (like there are topics that are unacceptable to treat with mockery) , when a joke can be morally and socially considered as acceptable I think it is mostly (if not entirely) subjective.Then you have Mary and Max, a film that I wouldn't know how to put it exactly in terms of its tone and intended comedy. I wouldn't say it is completely incorrect but it is unsettling and can be easily misguided making a pun out of the characters instead out of their actions as I guess is originally intended. I don't think it blatantly wants to mock suicide, mental illness, etc. but is exceedingly stupid the choices Adam Elliot picked for these topics and especially for some specific scenes. Take Mary's suicide sequence as an example, Mary having a fucking hill of pills to swallow gave me the same vibes as Tony Montana digging his face in a big mountain of coke at the end of Scarface, is so over the top and silly that it makes you question whether the director wants you to take the scene seriously or just crack up a laugh so as payoff it belittles the seriousness of the topic. And then there's the main topic the film tries to address: mental disorders. I don't have Asperger (the main topic this film addresses) so I'm not relating this over any personal experience over the subject but I found the portrayal of Max nauseating, his persona never exists outside Asperger, is as if he made Autism his whole personality and is never appreciated outside of that, the film choses that everyone (including himself) sees him as the \"Aspie\" guy so everything he does, feels and says is because of his mental condition. While yeah, a mental disorder can heavily condition a person it doesn't determines who that person is. If understanding someone with a mental condition is intended to make them comfortable and gain normality then is contradictory to pigeonhole them on a pre-stablished bias and reduce their person solely to their mental condition?As an example to understand this better, think on how several people react when someone announces he/she is homosexual. I've seen a surprising amount of people that when a celebrity states he is gay they automatically asume that everything he will say and how he will act is linked to his homosexuality, as if sexual preference was their whole personality; immediately they stop to see that person for his personality and just for being gay. Don't you think that simplifying a person to something they went through, a mental disorder, their sexual preference, etc. is making them feel pigeonholed on a square and forced to fill what society expects from them instead of truly accepting and supporting others by letting them be.This is how stereotypes are created, we perceive others not as persons but as ideals, nationalities, conditions, etc. And this is why Max doesn't work on my eyes, he is the embodiment of a stereotype of Asperger, I learned nothing of such an important topic for several people than what a Wikipedia article could already tell me, he is reduced to his mental condition and checks the boxes of the pre-conceived stereotype of Asperger. The thing is that we as individuals are so different that reducing something so complex to an academic reading or even worse to a stereotype feels dishonest to the real person's experience as we all might process and endure things differently.I've read some reviews of the site of people with Asperger that have mixed thoughts of this film, others have completely negative ones and others even loved it, as I said each person's personality and their way to react can be completely different to other's thus I find it fraudulent to make an unfair condition they can't change outweigh and determine the whole complex personality of a person.\n",
      "True rating: 1.0, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: life-is-beautiful\n",
      "Text: \"Silence is the most powerful cry.\"Finding comedy in a concentration camp. I'm just going to assume the audience behind this film's reception shares a one-circle Venn diagram with the audience behind Jojo Rabbit. Sadly, I cannot get behind a vanity project that humorously infantilizes and exploits the Holocaust / industrialized genocide as a Whimsical and Kitschy, melodramatic reinforcement of \"Smiling in the face of horror\" and \"You can get over it\" - or in the former film's case: \"Let everything happen to you. No feeling is final.\" If you ever want to make an inquiry into the fascism of sentimentality, you can start here. To Benigni, you can fuck right off, thanks.\n",
      "True rating: 0.5, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: sing-sing-2023\n",
      "Text: Really really impactful. Made me genuinely laugh out loud throughout but also always felt so sobering and therapeutic. And made me cry too, of course\n",
      "True rating: 4.0, Predicted rating: [5.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: napoleon\n",
      "Text: Week 20 of the 2nd Letterboxd Season ChallengeAn unseen movie from Paste Magazine's The 100 Best Silent Films of All Time listI'm sorry......but this is a 5 and a half hour movie.I know I chose to watch it but there was nothing else I really wanted to see from that list and since this is in the Letterboxd Top 250 (which I hope to complete someday) I figured I'd just watch this.There were some creative technical aspects and editing choices in this film which were way ahead of its time. There was also one really nice shot that made me go \"oooo\". But making me go \"oooo\" does not warrant 5 and a half hours of my time.It's like going through a really long vine compilation full of Jake Paul and Lele Pons just to get to the one vine where the little kid bellydances to the remix of Rihanna's song Monster.I can't really recommend this film to anyone unless you really want to know about the entire history of Napoleon but then again you could just read his wikipedia page you fucking nerds. This film is probably historically inaccurate anyway, where were they gonna get their information? The internet? It didn't exist!I'm just gonna add this last paragraph because I don't have enough of a following to be so insultingly sarcastic to a classic movie. They probably tried really hard.But maybe they should've just waited another 80 years or so before making such a big epic like this then it wouldn't have been so hard to make AND it would've been better.Better luck next time, you hacks.My 2nd Letterboxd Season Challenge1920s (Decade) Movies RankedTop 10 Longest Movies I've SeenTop 10 Worst Movies From 1920-1949Letterboxd Top 250 RankedSight & Sound Top 250 Ranked\n",
      "True rating: 2.5, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: mommy-2014\n",
      "Text: by the end of this i was one arm leaned up against the wall breathing like my ass just ran a 5k. i had so much i wanted to say as i was watching this, but now I'm just speechless and I want to lay down............ unbelievably beautiful in every way. amazing portrayal of dysfunctional relationships and the way we end up chasing the highs of the good times like a gambler desperate for a win just to break-even.Also not to indirect another review but someone called the inclusion of White Flag by Dido in this laughable or some shit idk I stopped reading but I'll have you know that song is incredible and warranted it's place in this film! I used to shower to that song everyday like 6 months ago and that shit saved my life\n",
      "True rating: 5.0, Predicted rating: [5.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: the-thing\n",
      "Text: 74%\"I don't know. Thousands of years ago it crashes, and this thing... gets thrown out, or crawls out, and it ends up freezing in the ice.\"Basically, Carpenter's cult horror film \"The Thing\" is a wintry Earth-based version of \"Alien\". Admittedly, I really like Ridley Scott's classic, so it only hurts more to say that The Thing left me quite a bit unsatisfied. Without a doubt what Carpenter has done here is impressive. By that I mean first and foremost the practical effects he and his team crafted together and that in the 80s. But the source of inspiration is clearly visible and I just had the feeling that the film lacks originality. Above all, it also takes the almost exact same problems from Alien (that I noticed) and continues them disastrously. And somehow it lacks a skillful suspense that I was waiting for all along. Also atmospherically the film could only convince me at times. In the whole it just doesn't want to work here, as many people can call the film a classic and masterpiece, as many say, I have no idea. One aspect in which both films have the same weakness is the supporting characters.  It always seems to me like they don't give a damn about them and they are just fodder for the aliens, which is true, but they treat them that way. The men of the research station are too poorly fleshed out and sometimes seem really strangely bland, without any charm. We don't get the chance to meet some faces at any time, because of course the main star (obviously who it is) has to be the main star. During the movie they mention a lot of names and I was sometimes completely triggered because I just didn't know who was who. They could have just as easily been called Doctor, Engineer, Flamethrower Man, Fireman, or Good Looking Guy of the movie and I would have figured out on my own which name goes with which character.All in all, the aliens are not only highly creatively designed but also slimy, disgusting and vomit worthy - as they should be. Carpenter stages a horror film the likes of which would probably not exist nowadays thanks to \"great\" computer technology. You can hardly find wet hands and an accelerated pulse, which is actually what I expect from this genre. All this is enough for a good film, but does not make a masterpiece for me. However, it is and remains cult!\n",
      "True rating: 3.5, Predicted rating: [0.5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: happy-together-1997\n",
      "Text: Film Club #20I am going to go ahead and give this film the 4 stars I think despite how much this film really wrecked my vibe and mood and I think I may hate it for that ?? But like this was a really interesting film, that takes complex themes, but wraps it into a pretty aimless story of toxic romance. The two leads are great and they are really doing some amazing work here. However, WKW must be commended for the strong direction and writing in this. It just has such a distinct style and tone that it really is unique. Overall, this has been one of the most interesting film club picks so far and I got to hand it to Lindsay for picking this, because I might not have watched it otherwise.\n",
      "True rating: 4.0, Predicted rating: [5.]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: the-best-of-youth\n",
      "Text: Spouse: “What are you watching this time?”Movie Obsessed Person: “A 6 hour Italian made-for-TV movie.”Spouse: looksMovie Obsessed Person: “It’s on the list.” I remember when The Best of Youth hit Netflix, it was a big deal in some circles. It currently sits at #114 on the Letterboxd Top 250 movies and has an 8.5 with 22,000 votes on IMDb. This film has made an impact on folks. It covers 4 decades in the life of an Italian family, focusing firmly on two brothers: Matteo & Nicola. In this sprawling familia epic we are introduced to our brothers at the end of an academic year. They are going before their professors for their final exams. Immediately we see that Matteo has an extensive internal life that is not shown to others, he is perhaps hiding himself from everyone. Nicola greets the world with an open mind, he seems more accepting of life on its terms. Matteo walks out of his exam when his professor disagrees with one of his opinions, while Nicola nabs an A. Matteo lashes out by going to an asylum and breaking a young patient out who has been subjected to electric shock therapy. As the brothers and two friends plan to head off to Norway for vacation, the two brothers take a detour to bring this young woman home to her family.Things of course go awry. The young woman had been placed there by her family and they were not prepared to take her in. Eventually, the police detain the woman and she is whisked back away into the system. The brothers are despondent. Matteo abandons Nicola and heads back toward Rome. Nicola forges on to Norway, but he doesn’t meet up with their friends and goes alone. At this point our brothers are literally on two different paths. Matteo abandons youth liberal optimism, joins the military and later the police force. He seems to seek external structure. While Nicola becomes a middle class vagabond in Norway, taking a gap year in school, grows a beard, sleeps with Scandinavians, and works in a lumber yard. Typical youthful rebellion stuff. I won’t go into detail on the rest. We follow them both, Matteo increases in anger and has outbursts throughout his career in the military/police, while Nicola opens up a progressive mental healthcare facility. Parents age and die. Sisters get married and divorced. Two friends pop back around eventually. A particularly melodramatic subplot with Nicola’s partner, a more dedicated communist than himself becomes a focal point for a while. Nothing about The Best of Youth struck me as bad other than the makeup jobs on the actors. When they had to age one that is either more youthful or elderly than their natural appearance, it is almost comical. It is a solid story that is well-acted. However, for a 6 hour film, it never creates enough depth in its characters. I never felt drawn into Matteo’s turmoil and why his anger seethed so deeply. I didn’t feel any nuance or purpose in Nicola’s subtle shifts of personal focus. Everything is broad, characters' emotions are stated blandly. Visually this is a good TV movie, but it isn’t more than that. Global: 3,408 (195 users)List of Shame: 1,770 < Black Robe> The Mermaid> Gesualdo: Death for Five Voices> Buck Rogers in the 25th Century> The Farmer Takes a Wife> Bloodsport> Star Trek: Generations> Star Trek IV: The Voyage Home> Multiplicity > Runaway Daughters> Made You Look< MankEntered Chart: 2,136 (50%)\n",
      "True rating: 3.0, Predicted rating: [5.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_single(review: str):\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN)\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.predict(Dataset.from_dict(inputs))\n",
    "        # print(f'predictions: {outputs.predictions}')\n",
    "        predicted_class_id = (outputs.predictions > 0.5).sum(axis=1)\n",
    "\n",
    "        return bin_to_rating(predicted_class_id)\n",
    "\n",
    "for example in dataset[\"test\"].to_list()[:10]:\n",
    "    # print(example)\n",
    "    text = example[\"text\"]\n",
    "    rating = example[\"rating\"]\n",
    "    movie = example.get(\"movie\", \"Unknown\")\n",
    "    pred = predict_single(text)\n",
    "    print(f'Movie: {movie}\\nText: {text}\\nTrue rating: {rating}, Predicted rating: {pred}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
