{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0a47fb-12c5-4028-9265-938b2d914b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "dataset = pd.read_csv(\"./letterboxd_250movie_reviews_normalized_sampled.csv\")\n",
    "model_path = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "id2label = {\n",
    "    0: \"½\",\n",
    "    1: \"★\",\n",
    "    2: \"★½\",\n",
    "    3: \"★★\",\n",
    "    4: \"★★½\",\n",
    "    5: \"★★★\",\n",
    "    6: \"★★★½\",\n",
    "    7: \"★★★★\",\n",
    "    8: \"★★★★½\",\n",
    "    9: \"★★★★★\",\n",
    "}\n",
    "label2id = {\n",
    "    \"½\": 0,\n",
    "    \"★\": 1,\n",
    "    \"★½\": 2,\n",
    "    \"★★\": 3,\n",
    "    \"★★½\": 4,\n",
    "    \"★★★\": 5,\n",
    "    \"★★★½\": 6,\n",
    "    \"★★★★\": 7,\n",
    "    \"★★★★½\": 8,\n",
    "    \"★★★★★\": 9,\n",
    "}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=10,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4454d484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b264bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input:\n",
      "Input IDs shape: torch.Size([1, 13])\n",
      "Attention mask shape: torch.Size([1, 13])\n",
      "\n",
      "Raw logits: tensor([[ 0.1682, -0.7521, -0.0206,  0.5124, -0.3735, -0.2041, -0.7227,  0.2623,\n",
      "         -0.4108,  0.1616]])\n",
      "Probabilities: tensor([[0.1255, 0.0500, 0.1039, 0.1770, 0.0730, 0.0865, 0.0515, 0.1378, 0.0703,\n",
      "         0.1246]])\n",
      "Predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "review = \"This movie was fantastic! Great acting and amazing plot.\"\n",
    "inputs = tokenizer(\n",
    "    review, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    ")\n",
    "print(\"Tokenized input:\")\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")\n",
    "\n",
    "# Make prediction (no gradient calculation needed for inference)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(f\"\\nRaw logits: {outputs.logits}\")\n",
    "print(f\"Probabilities: {predictions}\")\n",
    "print(f\"Predicted class: {torch.argmax(predictions, dim=-1).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447ea673",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"./data/letterboxd_250movie_reviews_test.csv\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_set[\"text\"].tolist(),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4906073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_labels = torch.argmax(predictions, dim=1)\n",
    "true_labels = torch.from_numpy(\n",
    "    test_set[\"label\"].to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2ba8f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Module inputs don't match the expected format.\nExpected format: {'references': Value('int32'), 'prediction_scores': Value('float32')},\nInput references: [4 2 1 0 7 4 9 6 7 5 2 3 0 4 0 3 0 6 8 8 2 3 8 2 1 5 6 4 2 1 2 4 3 0 2 2 9\n 3 4 0 7 7 6 6 3 2 8 9 6 9 6 8 0 4 8 8 7 4 8 7 0 6 6 0 2 6 5 7 1 9 1 9 6 6\n 7 8 9 2 8 2 6 3 8 4 2 4 8 4 6 5 4 0 7 1 3 1 8 3 8 9 0 2 7 2 0 1 4 6 7 0 6\n 6 0 1 0 6 8 9 2 7 0 0 1 5 9 5 3 7 5 2 8 9 8 2 8 5 9 3 9 2 1 0 6 0 3 5 0 1\n 7 1 2 3 9 1 3 2 0 4 7 6 7 0 3 2 8 9 5 7 6 8 3 5 8 3 0 0 8 2 5 4 9 4 3 6 5\n 9 0 8 5 7 9 4 6 4 6 6 5 3 0 6],\nInput prediction_scores: [[0.09784653 0.07659286 0.08248442 ... 0.13397627 0.071133   0.13050076]\n [0.1277937  0.05590996 0.0991604  ... 0.12975194 0.07107601 0.1238809 ]\n [0.1148041  0.06491779 0.08334477 ... 0.14174904 0.0662249  0.11772063]\n ...\n [0.14523251 0.03998476 0.10717452 ... 0.13240436 0.0682165  0.11420897]\n [0.12342191 0.06038117 0.08614881 ... 0.13221551 0.06676943 0.13375297]\n [0.10228305 0.06011888 0.11044272 ... 0.12273306 0.08708755 0.1221231 ]]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: acc, \u001b[33m\"\u001b[39m\u001b[33mAUC\u001b[39m\u001b[33m\"\u001b[39m: auc}\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# acc_result = accuracy.compute(\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#     predictions=predicted_labels, references=true_labels, normalize=True\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# mse = mean_squared_error(y_true=true_labels, y_pred=predicted_labels)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(\"MSE:\", mse)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcompute_metrics\u001b[39m\u001b[34m(eval_pred)\u001b[39m\n\u001b[32m      9\u001b[39m probabilities = np.exp(predictions) / np.exp(predictions).sum(-\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# compute multiclass auc using all class probabilities\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m auc = np.round(\u001b[43mauc_score\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43movr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mroc_auc\u001b[39m\u001b[33m'\u001b[39m],\u001b[32m3\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# predict most probable class\u001b[39;00m\n\u001b[32m     14\u001b[39m predicted_classes = np.argmax(predictions, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/evaluate/module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/ai-builders-week-5/.venv/lib/python3.13/site-packages/evaluate/module.py:546\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    540\u001b[39m     error_msg = (\n\u001b[32m    541\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and/or references don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.selected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    543\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    544\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Module inputs don't match the expected format.\nExpected format: {'references': Value('int32'), 'prediction_scores': Value('float32')},\nInput references: [4 2 1 0 7 4 9 6 7 5 2 3 0 4 0 3 0 6 8 8 2 3 8 2 1 5 6 4 2 1 2 4 3 0 2 2 9\n 3 4 0 7 7 6 6 3 2 8 9 6 9 6 8 0 4 8 8 7 4 8 7 0 6 6 0 2 6 5 7 1 9 1 9 6 6\n 7 8 9 2 8 2 6 3 8 4 2 4 8 4 6 5 4 0 7 1 3 1 8 3 8 9 0 2 7 2 0 1 4 6 7 0 6\n 6 0 1 0 6 8 9 2 7 0 0 1 5 9 5 3 7 5 2 8 9 8 2 8 5 9 3 9 2 1 0 6 0 3 5 0 1\n 7 1 2 3 9 1 3 2 0 4 7 6 7 0 3 2 8 9 5 7 6 8 3 5 8 3 0 0 8 2 5 4 9 4 3 6 5\n 9 0 8 5 7 9 4 6 4 6 6 5 3 0 6],\nInput prediction_scores: [[0.09784653 0.07659286 0.08248442 ... 0.13397627 0.071133   0.13050076]\n [0.1277937  0.05590996 0.0991604  ... 0.12975194 0.07107601 0.1238809 ]\n [0.1148041  0.06491779 0.08334477 ... 0.14174904 0.0662249  0.11772063]\n ...\n [0.14523251 0.03998476 0.10717452 ... 0.13240436 0.0682165  0.11420897]\n [0.12342191 0.06038117 0.08614881 ... 0.13221551 0.06676943 0.13375297]\n [0.10228305 0.06011888 0.11044272 ... 0.12273306 0.08708755 0.1221231 ]]"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # apply softmax to get probabilities\n",
    "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1, keepdims=True)\n",
    "    # compute multiclass auc using all class probabilities\n",
    "    auc = np.round(auc_score.compute(prediction_scores=probabilities, references=labels, multi_class=\"ovr\")['roc_auc'],3)\n",
    "    \n",
    "    # predict most probable class\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    # compute accuracy\n",
    "    acc = np.round(accuracy.compute(predictions=predicted_classes, references=labels)['accuracy'],3)\n",
    "    \n",
    "    return {\"Accuracy\": acc, \"AUC\": auc}\n",
    "\n",
    "# acc_result = accuracy.compute(\n",
    "#     predictions=predicted_labels, references=true_labels, normalize=True\n",
    "# )\n",
    "# print(\"Accuracy:\", acc_result['accuracy'])\n",
    "\n",
    "# mse = mean_squared_error(y_true=true_labels, y_pred=predicted_labels)\n",
    "# print(\"MSE:\", mse)\n",
    "compute_metrics((outputs.logits.numpy(), true_labels.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
